{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.12</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Warning: You are not running the latest version of PixieDust. Current is 1.1.12, Latest is 1.1.13</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>Please copy and run the following command in a new cell to upgrade: <span style=\"background-color:#ececec;font-family:monospace;padding:0 5px\">!pip install --user --upgrade pixiedust</span></div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Please restart kernel after upgrading.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import tweepy \n",
    "import time\n",
    "import datetime\n",
    "from tweepy import OAuthHandler\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "from IPython.display import clear_output\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from operator import attrgetter\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pixie_debugger\n",
    "  \n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        access_token_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "   \n",
    "    '''\n",
    "    Twitter only allows 100 tweets to be retrieved at a time.  This funciton\n",
    "    operates recursively to get the number of tweets requested in query_count.\n",
    "    '''\n",
    "    def __get_tweets_recursive(self, query, count = 10, collected_tweets=None, max_id=None, tweets_needed=0): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = []\n",
    "        ids=[]\n",
    "        fetched_tweets=[]\n",
    "        \n",
    "        # check count\n",
    "        if count < 10:\n",
    "            tweets_needed = count\n",
    "            count = 10\n",
    "        \n",
    "        query_count = count\n",
    "        \n",
    "        # clear_output()\n",
    "        print ('>> count: {}'.format(count))\n",
    "        print ('>> tweets_needed: {}'.format(tweets_needed))\n",
    "        print ('>> max_id: {}'.format(max_id))\n",
    "        try: \n",
    "            # call twitter api to fetch tweets\n",
    "            fetched_tweets = self.api.search(tweet_mode=\"extended\", max_id=max_id, q = query, count=query_count) \n",
    "            \n",
    "            # use tweets_needed to query large number of tweets\n",
    "            # even if we just need a few\n",
    "            if not tweets_needed:\n",
    "                tweets_needed = query_count\n",
    "            \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                \n",
    "                # keep track of ids for new queary \n",
    "                ids.append(tweet.id)\n",
    "                \n",
    "                # do a better check if tweet is english (first filtered in query but only to a point)\n",
    "                try:\n",
    "                    english = detect(tweet.full_text) == 'en'\n",
    "                except LangDetectException as le:\n",
    "                    # if anything other than english, assume not english\n",
    "                    print ('Language Exception: {}. Ignoring Tweet'.format(le))\n",
    "                    english = False\n",
    "   \n",
    "                if not english:\n",
    "                    continue\n",
    "                # only get long tweets\n",
    "                if len(tweet.full_text) < 100:\n",
    "                    continue\n",
    "                else: \n",
    "                    tweets.append(tweet)\n",
    "                    if len(tweets) == tweets_needed:\n",
    "                        print('DONE! Last {} tweets retrieved.'.format(len(tweets)))\n",
    "                        break\n",
    "        \n",
    "            # if did not get the number of tweets requested\n",
    "            if len(tweets) < tweets_needed:\n",
    "                new_query_count = int(tweets_needed) - len(tweets)\n",
    "                # use this to get new tweets  \n",
    "                max_id = min(ids)-1\n",
    "                \n",
    "                # this preserves the tweets_needed count for the next call\n",
    "                tweets_needed = new_query_count\n",
    "                \n",
    "                # this helps avoid several small queries\n",
    "                # query for more tweets than needed if need less than 10 \n",
    "                if new_query_count < 10:\n",
    "                    new_query_count = 10\n",
    "                 \n",
    "                print ('<< count: {}'.format(count))\n",
    "                print ('<< new_query_count: {}'.format(new_query_count))\n",
    "                print ('<< tweets_needed: {}'.format(tweets_needed))\n",
    "                print ('<< max_id: {}'.format(max_id))\n",
    "                print ('<< len(tweets): {}'.format(len(tweets)))\n",
    "                print ('===================')\n",
    "                more_tweets = self.__get_tweets_recursive(count=new_query_count,query=query,max_id=max_id, tweets_needed=tweets_needed)   \n",
    "                \n",
    "                # extends the list rather than appending the \n",
    "                # returned list object to the end of the list\n",
    "                tweets.extend(more_tweets)\n",
    "\n",
    "            # return parsed tweets\n",
    "            print ('Returning {} tweets'.format(len(tweets)))\n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "            \n",
    "        \n",
    "    def remove_duplicate_tweets(self, list_of_tweets):\n",
    "        \n",
    "        tweets_no_dups = []\n",
    "        seen_tweets_text = []\n",
    "        \n",
    "        for tweet in list_of_tweets:\n",
    "            if tweet.full_text not in seen_tweets_text:\n",
    "                tweets_no_dups.append(tweet)\n",
    "                seen_tweets_text.append(tweet.full_text)\n",
    "        \n",
    "        return tweets_no_dups\n",
    "        \n",
    "    \n",
    "    def get_tweets(self, query, count = 10, collected_tweets=None, max_id=None, tweets_needed=0): \n",
    "        \n",
    "        tweets_check_for_dups = self.__get_tweets_recursive(query=query, \n",
    "                                                count = count, \n",
    "                                                collected_tweets = collected_tweets, \n",
    "                                                max_id = max_id, \n",
    "                                                tweets_needed = tweets_needed)\n",
    "          \n",
    "        tweets_no_dups = self.remove_duplicate_tweets(tweets_check_for_dups) \n",
    "        \n",
    "        number_of_dups = len(tweets_check_for_dups) - len(tweets_no_dups)\n",
    "       \n",
    "        while number_of_dups > 0:\n",
    "            \n",
    "            print ('{} Duplicates Found'.format(number_of_dups))\n",
    "            \n",
    "            # get lowest tweet id\n",
    "            min_num = min(tweets_check_for_dups,key=attrgetter('id'))\n",
    "            max_id = min_num.id - 1\n",
    "            \n",
    "            # get tweets to replace the duplicates\n",
    "            replace_dup_tweets = self.__get_tweets_recursive(query, count = count, \n",
    "                                                collected_tweets = collected_tweets, \n",
    "                                                max_id = max_id, \n",
    "                                                tweets_needed = number_of_dups)\n",
    "            \n",
    "            # add the replacement tweets to the list of unique tweets\n",
    "            # replacements may have dups, will need to re-check\n",
    "            tweets_check_for_dups = tweets_no_dups\n",
    "            tweets_check_for_dups.extend(replace_dup_tweets)\n",
    "            \n",
    "            # check this new list for any duplicates\n",
    "            tweets_no_dups = self.remove_duplicate_tweets(tweets_check_for_dups)\n",
    "                    \n",
    "            # if there are any more duplicates, go through the loop again\n",
    "            number_of_dups = len(tweets_check_for_dups) - len(tweets_no_dups)\n",
    "\n",
    "        \n",
    "        return tweets_check_for_dups\n",
    "        \n",
    "        # Check for duplicate tweets, these could be across\n",
    "        # multiple batches; better to just check the entire set rather\n",
    "        # as we recursively grab them\n",
    "        \n",
    "        \n",
    "        # Get remaining tweets starting from the lowest tweet retrieved\n",
    "     \n",
    "            \n",
    "# creating object of TwitterClient Class \n",
    "api = TwitterClient() \n",
    "# calling function to get tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> count: 1000\n",
      ">> tweets_needed: 0\n",
      ">> max_id: None\n",
      "<< count: 1000\n",
      "<< new_query_count: 930\n",
      "<< tweets_needed: 930\n",
      "<< max_id: 1054147574930694143\n",
      "<< len(tweets): 70\n",
      "===================\n",
      ">> count: 930\n",
      ">> tweets_needed: 930\n",
      ">> max_id: 1054147574930694143\n",
      "<< count: 930\n",
      "<< new_query_count: 867\n",
      "<< tweets_needed: 867\n",
      "<< max_id: 1054030640730066944\n",
      "<< len(tweets): 63\n",
      "===================\n",
      ">> count: 867\n",
      ">> tweets_needed: 867\n",
      ">> max_id: 1054030640730066944\n",
      "<< count: 867\n",
      "<< new_query_count: 806\n",
      "<< tweets_needed: 806\n",
      "<< max_id: 1053797323350786048\n",
      "<< len(tweets): 61\n",
      "===================\n",
      ">> count: 806\n",
      ">> tweets_needed: 806\n",
      ">> max_id: 1053797323350786048\n",
      "<< count: 806\n",
      "<< new_query_count: 751\n",
      "<< tweets_needed: 751\n",
      "<< max_id: 1053659807419457536\n",
      "<< len(tweets): 55\n",
      "===================\n",
      ">> count: 751\n",
      ">> tweets_needed: 751\n",
      ">> max_id: 1053659807419457536\n",
      "<< count: 751\n",
      "<< new_query_count: 688\n",
      "<< tweets_needed: 688\n",
      "<< max_id: 1053447310179528705\n",
      "<< len(tweets): 63\n",
      "===================\n",
      ">> count: 688\n",
      ">> tweets_needed: 688\n",
      ">> max_id: 1053447310179528705\n",
      "<< count: 688\n",
      "<< new_query_count: 625\n",
      "<< tweets_needed: 625\n",
      "<< max_id: 1053275591926001666\n",
      "<< len(tweets): 63\n",
      "===================\n",
      ">> count: 625\n",
      ">> tweets_needed: 625\n",
      ">> max_id: 1053275591926001666\n",
      "<< count: 625\n",
      "<< new_query_count: 561\n",
      "<< tweets_needed: 561\n",
      "<< max_id: 1053061535822684161\n",
      "<< len(tweets): 64\n",
      "===================\n",
      ">> count: 561\n",
      ">> tweets_needed: 561\n",
      ">> max_id: 1053061535822684161\n",
      "<< count: 561\n",
      "<< new_query_count: 502\n",
      "<< tweets_needed: 502\n",
      "<< max_id: 1052920762104012804\n",
      "<< len(tweets): 59\n",
      "===================\n",
      ">> count: 502\n",
      ">> tweets_needed: 502\n",
      ">> max_id: 1052920762104012804\n",
      "<< count: 502\n",
      "<< new_query_count: 438\n",
      "<< tweets_needed: 438\n",
      "<< max_id: 1052727804658442239\n",
      "<< len(tweets): 64\n",
      "===================\n",
      ">> count: 438\n",
      ">> tweets_needed: 438\n",
      ">> max_id: 1052727804658442239\n",
      "<< count: 438\n",
      "<< new_query_count: 376\n",
      "<< tweets_needed: 376\n",
      "<< max_id: 1052590378656907263\n",
      "<< len(tweets): 62\n",
      "===================\n",
      ">> count: 376\n",
      ">> tweets_needed: 376\n",
      ">> max_id: 1052590378656907263\n",
      "<< count: 376\n",
      "<< new_query_count: 308\n",
      "<< tweets_needed: 308\n",
      "<< max_id: 1052411985957130242\n",
      "<< len(tweets): 68\n",
      "===================\n",
      ">> count: 308\n",
      ">> tweets_needed: 308\n",
      ">> max_id: 1052411985957130242\n",
      "<< count: 308\n",
      "<< new_query_count: 250\n",
      "<< tweets_needed: 250\n",
      "<< max_id: 1052290973974319105\n",
      "<< len(tweets): 58\n",
      "===================\n",
      ">> count: 250\n",
      ">> tweets_needed: 250\n",
      ">> max_id: 1052290973974319105\n",
      "<< count: 250\n",
      "<< new_query_count: 192\n",
      "<< tweets_needed: 192\n",
      "<< max_id: 1052114125084454911\n",
      "<< len(tweets): 58\n",
      "===================\n",
      ">> count: 192\n",
      ">> tweets_needed: 192\n",
      ">> max_id: 1052114125084454911\n",
      "<< count: 192\n",
      "<< new_query_count: 139\n",
      "<< tweets_needed: 139\n",
      "<< max_id: 1051959536398028800\n",
      "<< len(tweets): 53\n",
      "===================\n",
      ">> count: 139\n",
      ">> tweets_needed: 139\n",
      ">> max_id: 1051959536398028800\n",
      "<< count: 139\n",
      "<< new_query_count: 80\n",
      "<< tweets_needed: 80\n",
      "<< max_id: 1051843466303496191\n",
      "<< len(tweets): 59\n",
      "===================\n",
      ">> count: 80\n",
      ">> tweets_needed: 80\n",
      ">> max_id: 1051843466303496191\n",
      "<< count: 80\n",
      "<< new_query_count: 33\n",
      "<< tweets_needed: 33\n",
      "<< max_id: 1051646859830353919\n",
      "<< len(tweets): 47\n",
      "===================\n",
      ">> count: 33\n",
      ">> tweets_needed: 33\n",
      ">> max_id: 1051646859830353919\n",
      "<< count: 33\n",
      "<< new_query_count: 18\n",
      "<< tweets_needed: 18\n",
      "<< max_id: 1051616073882120191\n",
      "<< len(tweets): 15\n",
      "===================\n",
      ">> count: 18\n",
      ">> tweets_needed: 18\n",
      ">> max_id: 1051616073882120191\n",
      "<< count: 18\n",
      "<< new_query_count: 11\n",
      "<< tweets_needed: 11\n",
      "<< max_id: 1051598923540963327\n",
      "<< len(tweets): 7\n",
      "===================\n",
      ">> count: 11\n",
      ">> tweets_needed: 11\n",
      ">> max_id: 1051598923540963327\n",
      "<< count: 11\n",
      "<< new_query_count: 10\n",
      "<< tweets_needed: 5\n",
      "<< max_id: 1051587321814908927\n",
      "<< len(tweets): 6\n",
      "===================\n",
      ">> count: 10\n",
      ">> tweets_needed: 5\n",
      ">> max_id: 1051587321814908927\n",
      "DONE! Last 5 tweets retrieved.\n",
      "Returning 5 tweets\n",
      "Returning 11 tweets\n",
      "Returning 18 tweets\n",
      "Returning 33 tweets\n",
      "Returning 80 tweets\n",
      "Returning 139 tweets\n",
      "Returning 192 tweets\n",
      "Returning 250 tweets\n",
      "Returning 308 tweets\n",
      "Returning 376 tweets\n",
      "Returning 438 tweets\n",
      "Returning 502 tweets\n",
      "Returning 561 tweets\n",
      "Returning 625 tweets\n",
      "Returning 688 tweets\n",
      "Returning 751 tweets\n",
      "Returning 806 tweets\n",
      "Returning 867 tweets\n",
      "Returning 930 tweets\n",
      "Returning 1000 tweets\n",
      "30 Duplicates Found\n",
      ">> count: 1000\n",
      ">> tweets_needed: 30\n",
      ">> max_id: 1051572477762113535\n",
      "DONE! Last 30 tweets retrieved.\n",
      "Returning 30 tweets\n",
      "1 Duplicates Found\n",
      ">> count: 1000\n",
      ">> tweets_needed: 1\n",
      ">> max_id: 1051493382810419199\n",
      "DONE! Last 1 tweets retrieved.\n",
      "Returning 1 tweets\n"
     ]
    }
   ],
   "source": [
    "sarcastic_tweets = api.get_tweets(count = 1000 , query = '#sarcasm -filter:links -filter:media filter:safe -filter:retweets lang:en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sarcastic_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tweet_texts = []\n",
    "s_tweet_ids = []\n",
    "for s_tweet in sarcastic_tweets:\n",
    "    s_tweet_texts.append(s_tweet.full_text)\n",
    "    s_tweet_ids.append(s_tweet.id)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1054369341704228869</td>\n",
       "      <td>@jeffrichadiha Which one of the 5 touchdowns d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1054363559709327360</td>\n",
       "      <td>@SimmsYanne Everyone loves the DMV! I go there...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1054362726586187776</td>\n",
       "      <td>This no #sleep thing is my favorite. Its not i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1054362006013140992</td>\n",
       "      <td>@JamesKosur And today, the body double. I'm su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1054359096629940224</td>\n",
       "      <td>Some people write TV coz they can't write Teli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                             tweets  \\\n",
       "0  1054369341704228869  @jeffrichadiha Which one of the 5 touchdowns d...   \n",
       "1  1054363559709327360  @SimmsYanne Everyone loves the DMV! I go there...   \n",
       "2  1054362726586187776  This no #sleep thing is my favorite. Its not i...   \n",
       "3  1054362006013140992  @JamesKosur And today, the body double. I'm su...   \n",
       "4  1054359096629940224  Some people write TV coz they can't write Teli...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st = pd.DataFrame()\n",
    "df_st['id'] = s_tweet_ids\n",
    "df_st['tweets']=s_tweet_texts\n",
    "df_st['label'] = 1\n",
    "df_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> count: 1000\n",
      ">> tweets_needed: 0\n",
      ">> max_id: None\n",
      "<< count: 1000\n",
      "<< new_query_count: 963\n",
      "<< tweets_needed: 963\n",
      "<< max_id: 1054370517409521664\n",
      "<< len(tweets): 37\n",
      "===================\n",
      ">> count: 963\n",
      ">> tweets_needed: 963\n",
      ">> max_id: 1054370517409521664\n",
      "<< count: 963\n",
      "<< new_query_count: 926\n",
      "<< tweets_needed: 926\n",
      "<< max_id: 1054370515966664704\n",
      "<< len(tweets): 37\n",
      "===================\n",
      ">> count: 926\n",
      ">> tweets_needed: 926\n",
      ">> max_id: 1054370515966664704\n",
      "<< count: 926\n",
      "<< new_query_count: 899\n",
      "<< tweets_needed: 899\n",
      "<< max_id: 1054370514452561919\n",
      "<< len(tweets): 27\n",
      "===================\n",
      ">> count: 899\n",
      ">> tweets_needed: 899\n",
      ">> max_id: 1054370514452561919\n",
      "<< count: 899\n",
      "<< new_query_count: 875\n",
      "<< tweets_needed: 875\n",
      "<< max_id: 1054370512799854591\n",
      "<< len(tweets): 24\n",
      "===================\n",
      ">> count: 875\n",
      ">> tweets_needed: 875\n",
      ">> max_id: 1054370512799854591\n",
      "<< count: 875\n",
      "<< new_query_count: 847\n",
      "<< tweets_needed: 847\n",
      "<< max_id: 1054370511365394431\n",
      "<< len(tweets): 28\n",
      "===================\n",
      ">> count: 847\n",
      ">> tweets_needed: 847\n",
      ">> max_id: 1054370511365394431\n",
      "<< count: 847\n",
      "<< new_query_count: 817\n",
      "<< tweets_needed: 817\n",
      "<< max_id: 1054370509591363584\n",
      "<< len(tweets): 30\n",
      "===================\n",
      ">> count: 817\n",
      ">> tweets_needed: 817\n",
      ">> max_id: 1054370509591363584\n",
      "<< count: 817\n",
      "<< new_query_count: 784\n",
      "<< tweets_needed: 784\n",
      "<< max_id: 1054370508160954367\n",
      "<< len(tweets): 33\n",
      "===================\n",
      ">> count: 784\n",
      ">> tweets_needed: 784\n",
      ">> max_id: 1054370508160954367\n",
      "<< count: 784\n",
      "<< new_query_count: 751\n",
      "<< tweets_needed: 751\n",
      "<< max_id: 1054370506680516607\n",
      "<< len(tweets): 33\n",
      "===================\n",
      ">> count: 751\n",
      ">> tweets_needed: 751\n",
      ">> max_id: 1054370506680516607\n",
      "<< count: 751\n",
      "<< new_query_count: 719\n",
      "<< tweets_needed: 719\n",
      "<< max_id: 1054370505187381248\n",
      "<< len(tweets): 32\n",
      "===================\n",
      ">> count: 719\n",
      ">> tweets_needed: 719\n",
      ">> max_id: 1054370505187381248\n",
      "<< count: 719\n",
      "<< new_query_count: 694\n",
      "<< tweets_needed: 694\n",
      "<< max_id: 1054370503769669631\n",
      "<< len(tweets): 25\n",
      "===================\n",
      ">> count: 694\n",
      ">> tweets_needed: 694\n",
      ">> max_id: 1054370503769669631\n",
      "<< count: 694\n",
      "<< new_query_count: 666\n",
      "<< tweets_needed: 666\n",
      "<< max_id: 1054370502268071935\n",
      "<< len(tweets): 28\n",
      "===================\n",
      ">> count: 666\n",
      ">> tweets_needed: 666\n",
      ">> max_id: 1054370502268071935\n",
      "<< count: 666\n",
      "<< new_query_count: 642\n",
      "<< tweets_needed: 642\n",
      "<< max_id: 1054370500552638466\n",
      "<< len(tweets): 24\n",
      "===================\n",
      ">> count: 642\n",
      ">> tweets_needed: 642\n",
      ">> max_id: 1054370500552638466\n",
      "<< count: 642\n",
      "<< new_query_count: 615\n",
      "<< tweets_needed: 615\n",
      "<< max_id: 1054370498916896768\n",
      "<< len(tweets): 27\n",
      "===================\n",
      ">> count: 615\n",
      ">> tweets_needed: 615\n",
      ">> max_id: 1054370498916896768\n",
      "<< count: 615\n",
      "<< new_query_count: 585\n",
      "<< tweets_needed: 585\n",
      "<< max_id: 1054370497503342597\n",
      "<< len(tweets): 30\n",
      "===================\n",
      ">> count: 585\n",
      ">> tweets_needed: 585\n",
      ">> max_id: 1054370497503342597\n",
      "<< count: 585\n",
      "<< new_query_count: 558\n",
      "<< tweets_needed: 558\n",
      "<< max_id: 1054370495854927872\n",
      "<< len(tweets): 27\n",
      "===================\n",
      ">> count: 558\n",
      ">> tweets_needed: 558\n",
      ">> max_id: 1054370495854927872\n",
      "<< count: 558\n",
      "<< new_query_count: 527\n",
      "<< tweets_needed: 527\n",
      "<< max_id: 1054370493980180479\n",
      "<< len(tweets): 31\n",
      "===================\n",
      ">> count: 527\n",
      ">> tweets_needed: 527\n",
      ">> max_id: 1054370493980180479\n",
      "<< count: 527\n",
      "<< new_query_count: 495\n",
      "<< tweets_needed: 495\n",
      "<< max_id: 1054370492440764420\n",
      "<< len(tweets): 32\n",
      "===================\n",
      ">> count: 495\n",
      ">> tweets_needed: 495\n",
      ">> max_id: 1054370492440764420\n",
      "<< count: 495\n",
      "<< new_query_count: 470\n",
      "<< tweets_needed: 470\n",
      "<< max_id: 1054370490461155328\n",
      "<< len(tweets): 25\n",
      "===================\n",
      ">> count: 470\n",
      ">> tweets_needed: 470\n",
      ">> max_id: 1054370490461155328\n",
      "<< count: 470\n",
      "<< new_query_count: 447\n",
      "<< tweets_needed: 447\n",
      "<< max_id: 1054370488833794052\n",
      "<< len(tweets): 23\n",
      "===================\n",
      ">> count: 447\n",
      ">> tweets_needed: 447\n",
      ">> max_id: 1054370488833794052\n",
      "<< count: 447\n",
      "<< new_query_count: 422\n",
      "<< tweets_needed: 422\n",
      "<< max_id: 1054370487072186367\n",
      "<< len(tweets): 25\n",
      "===================\n",
      ">> count: 422\n",
      ">> tweets_needed: 422\n",
      ">> max_id: 1054370487072186367\n",
      "<< count: 422\n",
      "<< new_query_count: 391\n",
      "<< tweets_needed: 391\n",
      "<< max_id: 1054370485587329023\n",
      "<< len(tweets): 31\n",
      "===================\n",
      ">> count: 391\n",
      ">> tweets_needed: 391\n",
      ">> max_id: 1054370485587329023\n",
      "<< count: 391\n",
      "<< new_query_count: 365\n",
      "<< tweets_needed: 365\n",
      "<< max_id: 1054370483922198527\n",
      "<< len(tweets): 26\n",
      "===================\n",
      ">> count: 365\n",
      ">> tweets_needed: 365\n",
      ">> max_id: 1054370483922198527\n",
      "<< count: 365\n",
      "<< new_query_count: 339\n",
      "<< tweets_needed: 339\n",
      "<< max_id: 1054370482496114687\n",
      "<< len(tweets): 26\n",
      "===================\n",
      ">> count: 339\n",
      ">> tweets_needed: 339\n",
      ">> max_id: 1054370482496114687\n",
      "<< count: 339\n",
      "<< new_query_count: 310\n",
      "<< tweets_needed: 310\n",
      "<< max_id: 1054370481120337921\n",
      "<< len(tweets): 29\n",
      "===================\n",
      ">> count: 310\n",
      ">> tweets_needed: 310\n",
      ">> max_id: 1054370481120337921\n",
      "<< count: 310\n",
      "<< new_query_count: 279\n",
      "<< tweets_needed: 279\n",
      "<< max_id: 1054370479878758400\n",
      "<< len(tweets): 31\n",
      "===================\n",
      ">> count: 279\n",
      ">> tweets_needed: 279\n",
      ">> max_id: 1054370479878758400\n",
      "<< count: 279\n",
      "<< new_query_count: 244\n",
      "<< tweets_needed: 244\n",
      "<< max_id: 1054370478297698303\n",
      "<< len(tweets): 35\n",
      "===================\n",
      ">> count: 244\n",
      ">> tweets_needed: 244\n",
      ">> max_id: 1054370478297698303\n",
      "<< count: 244\n",
      "<< new_query_count: 219\n",
      "<< tweets_needed: 219\n",
      "<< max_id: 1054370476678475775\n",
      "<< len(tweets): 25\n",
      "===================\n",
      ">> count: 219\n",
      ">> tweets_needed: 219\n",
      ">> max_id: 1054370476678475775\n",
      "<< count: 219\n",
      "<< new_query_count: 191\n",
      "<< tweets_needed: 191\n",
      "<< max_id: 1054370475164532735\n",
      "<< len(tweets): 28\n",
      "===================\n",
      ">> count: 191\n",
      ">> tweets_needed: 191\n",
      ">> max_id: 1054370475164532735\n",
      "<< count: 191\n",
      "<< new_query_count: 156\n",
      "<< tweets_needed: 156\n",
      "<< max_id: 1054370473499287553\n",
      "<< len(tweets): 35\n",
      "===================\n",
      ">> count: 156\n",
      ">> tweets_needed: 156\n",
      ">> max_id: 1054370473499287553\n",
      "<< count: 156\n",
      "<< new_query_count: 121\n",
      "<< tweets_needed: 121\n",
      "<< max_id: 1054370471905583108\n",
      "<< len(tweets): 35\n",
      "===================\n",
      ">> count: 121\n",
      ">> tweets_needed: 121\n",
      ">> max_id: 1054370471905583108\n",
      "<< count: 121\n",
      "<< new_query_count: 86\n",
      "<< tweets_needed: 86\n",
      "<< max_id: 1054370470307487744\n",
      "<< len(tweets): 35\n",
      "===================\n",
      ">> count: 86\n",
      ">> tweets_needed: 86\n",
      ">> max_id: 1054370470307487744\n",
      "<< count: 86\n",
      "<< new_query_count: 65\n",
      "<< tweets_needed: 65\n",
      "<< max_id: 1054370468986142721\n",
      "<< len(tweets): 21\n",
      "===================\n",
      ">> count: 65\n",
      ">> tweets_needed: 65\n",
      ">> max_id: 1054370468986142721\n",
      "<< count: 65\n",
      "<< new_query_count: 40\n",
      "<< tweets_needed: 40\n",
      "<< max_id: 1054370468029980672\n",
      "<< len(tweets): 25\n",
      "===================\n",
      ">> count: 40\n",
      ">> tweets_needed: 40\n",
      ">> max_id: 1054370468029980672\n",
      "<< count: 40\n",
      "<< new_query_count: 27\n",
      "<< tweets_needed: 27\n",
      "<< max_id: 1054370467602161665\n",
      "<< len(tweets): 13\n",
      "===================\n",
      ">> count: 27\n",
      ">> tweets_needed: 27\n",
      ">> max_id: 1054370467602161665\n",
      "<< count: 27\n",
      "<< new_query_count: 17\n",
      "<< tweets_needed: 17\n",
      "<< max_id: 1054370467111292927\n",
      "<< len(tweets): 10\n",
      "===================\n",
      ">> count: 17\n",
      ">> tweets_needed: 17\n",
      ">> max_id: 1054370467111292927\n",
      "<< count: 17\n",
      "<< new_query_count: 10\n",
      "<< tweets_needed: 10\n",
      "<< max_id: 1054370466893361151\n",
      "<< len(tweets): 7\n",
      "===================\n",
      ">> count: 10\n",
      ">> tweets_needed: 10\n",
      ">> max_id: 1054370466893361151\n",
      "<< count: 10\n",
      "<< new_query_count: 10\n",
      "<< tweets_needed: 10\n",
      "<< max_id: 1054370466771689471\n",
      "<< len(tweets): 0\n",
      "===================\n",
      ">> count: 10\n",
      ">> tweets_needed: 10\n",
      ">> max_id: 1054370466771689471\n",
      "<< count: 10\n",
      "<< new_query_count: 10\n",
      "<< tweets_needed: 9\n",
      "<< max_id: 1054370466616434688\n",
      "<< len(tweets): 1\n",
      "===================\n",
      ">> count: 10\n",
      ">> tweets_needed: 9\n",
      ">> max_id: 1054370466616434688\n",
      "<< count: 10\n",
      "<< new_query_count: 10\n",
      "<< tweets_needed: 7\n",
      "<< max_id: 1054370466457038850\n",
      "<< len(tweets): 2\n",
      "===================\n",
      ">> count: 10\n",
      ">> tweets_needed: 7\n",
      ">> max_id: 1054370466457038850\n",
      "<< count: 10\n",
      "<< new_query_count: 10\n",
      "<< tweets_needed: 4\n",
      "<< max_id: 1054370466176077823\n",
      "<< len(tweets): 3\n",
      "===================\n",
      ">> count: 10\n",
      ">> tweets_needed: 4\n",
      ">> max_id: 1054370466176077823\n",
      "DONE! Last 4 tweets retrieved.\n",
      "Returning 4 tweets\n",
      "Returning 7 tweets\n",
      "Returning 9 tweets\n",
      "Returning 10 tweets\n",
      "Returning 10 tweets\n",
      "Returning 17 tweets\n",
      "Returning 27 tweets\n",
      "Returning 40 tweets\n",
      "Returning 65 tweets\n",
      "Returning 86 tweets\n",
      "Returning 121 tweets\n",
      "Returning 156 tweets\n",
      "Returning 191 tweets\n",
      "Returning 219 tweets\n",
      "Returning 244 tweets\n",
      "Returning 279 tweets\n",
      "Returning 310 tweets\n",
      "Returning 339 tweets\n",
      "Returning 365 tweets\n",
      "Returning 391 tweets\n",
      "Returning 422 tweets\n",
      "Returning 447 tweets\n",
      "Returning 470 tweets\n",
      "Returning 495 tweets\n",
      "Returning 527 tweets\n",
      "Returning 558 tweets\n",
      "Returning 585 tweets\n",
      "Returning 615 tweets\n",
      "Returning 642 tweets\n",
      "Returning 666 tweets\n",
      "Returning 694 tweets\n",
      "Returning 719 tweets\n",
      "Returning 751 tweets\n",
      "Returning 784 tweets\n",
      "Returning 817 tweets\n",
      "Returning 847 tweets\n",
      "Returning 875 tweets\n",
      "Returning 899 tweets\n",
      "Returning 926 tweets\n",
      "Returning 963 tweets\n",
      "Returning 1000 tweets\n",
      "2 Duplicates Found\n",
      ">> count: 1000\n",
      ">> tweets_needed: 2\n",
      ">> max_id: 1054370466062712832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! Last 2 tweets retrieved.\n",
      "Returning 2 tweets\n"
     ]
    }
   ],
   "source": [
    "nonsarcastic_tweets = api.get_tweets(count = 1000, query = '-#sarcasm -#sarcastic -filter:links -filter:media filter:safe -filter:retweets lang:en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1054370518969679872</td>\n",
       "      <td>@heydimpIes exactly thats why im like literall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1054370518927843328</td>\n",
       "      <td>@KirenRijiju @narendramodi @Ra_THORe @BJP4Indi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1054370518898540545</td>\n",
       "      <td>Cancel culture has only successfully canceled ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1054370518860812288</td>\n",
       "      <td>I suddenly felt bad for all the mean things i ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1054370518814490624</td>\n",
       "      <td>@V_of_Europe Canada  is following Sweden with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                             tweets  \\\n",
       "0  1054370518969679872  @heydimpIes exactly thats why im like literall...   \n",
       "1  1054370518927843328  @KirenRijiju @narendramodi @Ra_THORe @BJP4Indi...   \n",
       "2  1054370518898540545  Cancel culture has only successfully canceled ...   \n",
       "3  1054370518860812288  I suddenly felt bad for all the mean things i ...   \n",
       "4  1054370518814490624  @V_of_Europe Canada  is following Sweden with ...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns_tweet_texts = []\n",
    "ns_tweet_ids = []\n",
    "for ns_tweet in nonsarcastic_tweets:\n",
    "    ns_tweet_texts.append(ns_tweet.full_text)\n",
    "    ns_tweet_ids.append(ns_tweet.id) \n",
    "\n",
    "df_nst = pd.DataFrame()\n",
    "df_nst['id'] = ns_tweet_ids\n",
    "df_nst['tweets'] = ns_tweet_texts\n",
    "df_nst['label'] = 0\n",
    "df_nst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_st_nst = pd.concat([df_st, df_nst], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_stamp = datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'sarcastic_' + str(len(df_st_nst)) + '_' + date_stamp + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcastic_2000_20181022_1356.csv\n"
     ]
    }
   ],
   "source": [
    "print (file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_st_nst.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Combine sarcastic file contents\n",
    "import glob\n",
    "allFiles = glob.glob('sarcastic_2000*.csv')\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>@mitchellreports @ChuckGrassley @SenKamalaHarr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>@paintingcorner You mean average followed by h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>@elizabethforma Probably should start handing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>@SenBlumenthal In Roman times a loss like this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I love #sarcasm just short of passive aggressi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label                                             tweets\n",
       "0  0.0      1  @mitchellreports @ChuckGrassley @SenKamalaHarr...\n",
       "1  0.0      1  @paintingcorner You mean average followed by h...\n",
       "2  0.0      1  @elizabethforma Probably should start handing ...\n",
       "3  0.0      1  @SenBlumenthal In Roman times a loss like this...\n",
       "4  0.0      1  I love #sarcasm just short of passive aggressi..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_records(tweet_df):\n",
    "\n",
    "    tweets_no_dups = pd.DataFrame()\n",
    "    seen_tweets_text = []\n",
    "    dup_count = 0\n",
    "\n",
    "    for index,row in tweet_df.iterrows():\n",
    "        \n",
    "        if row['tweets'] not in seen_tweets_text:\n",
    "            seen_tweets_text.append(row['tweets'])\n",
    "            tweets_no_dups = tweets_no_dups.append(row)\n",
    "        else:\n",
    "            dup_count += 1\n",
    "            clear_output()\n",
    "            print ('Duplicates: {}'.format(dup_count))\n",
    "            print ('{}'.format(row['tweets']))\n",
    "        \n",
    "\n",
    "    return tweets_no_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 10173\n",
      "I cast my Heisman ballot for #EdOliver! Click the player you think deserves the Heisman House vote. (📍@NissanUSA)\n"
     ]
    }
   ],
   "source": [
    "nodups = pd.DataFrame()\n",
    "nodups = remove_duplicate_records(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@mitchellreports @ChuckGrassley @SenKamalaHarr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@paintingcorner You mean average followed by h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@elizabethforma Probably should start handing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@SenBlumenthal In Roman times a loss like this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I love #sarcasm just short of passive aggressi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label                                             tweets\n",
       "0  0.0    1.0  @mitchellreports @ChuckGrassley @SenKamalaHarr...\n",
       "1  0.0    1.0  @paintingcorner You mean average followed by h...\n",
       "2  0.0    1.0  @elizabethforma Probably should start handing ...\n",
       "3  0.0    1.0  @SenBlumenthal In Roman times a loss like this...\n",
       "4  0.0    1.0  I love #sarcasm just short of passive aggressi..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an even file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarc_only = pd.DataFrame()\n",
    "sarc_only = nodups[nodups['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2855\n"
     ]
    }
   ],
   "source": [
    "sarc_len = len(sarc_only)\n",
    "print (sarc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10972"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodups[nodups['label']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonsarc_even_sample = nodups[nodups['label']==0].sample(n=sarc_len, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_sarc_nonsarc = sarc_only.append(nonsarc_even_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5710"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(even_sarc_nonsarc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even_sarc_5710_20181022_1524.csv\n"
     ]
    }
   ],
   "source": [
    "date_stamp = datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
    "file_name = 'even_sarc_' + str(len(even_sarc_nonsarc)) + '_' + date_stamp + '.csv'\n",
    "print (file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "even_sarc_nonsarc.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
